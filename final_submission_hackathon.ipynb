{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4 week DL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfMK5FEhUGde"
      },
      "source": [
        "## Download the images\n",
        "\n",
        "\n",
        "We can use **GoogleDriveDownloader** form **google_drive_downloader** library in Python to download the shared files from the shared Google drive link: https://drive.google.com/file/d/1f7uslI-ZHidriQFZR966_aILjlkgDN76/view?usp=sharing\n",
        "\n",
        "The file id in the above link is: **1f7uslI-ZHidriQFZR966_aILjlkgDN76**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_1BysB4Re7f",
        "outputId": "c0d793e9-b97f-4167-fab9-d811f9cd628b"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1f7uslI-ZHidriQFZR966_aILjlkgDN76',\n",
        "                                    dest_path='content/eye_gender_data.zip',\n",
        "                                    unzip=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1f7uslI-ZHidriQFZR966_aILjlkgDN76 into content/eye_gender_data.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM_FDQgXbh_-"
      },
      "source": [
        "We have all the files from the shared Google drive link downloaded in the colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsIqjb7Ebs3B"
      },
      "source": [
        "## Loading Libraries\n",
        "All Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n",
        "\n",
        "We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd, tensorlow --> tf).\n",
        "\n",
        "Note: You can import all the libraries that you think will be required or can import it as you go along."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIe16kmoUmhr"
      },
      "source": [
        "import pandas as pd                                     # Data analysis and manipultion tool\n",
        "import numpy as np                                      # Fundamental package for linear algebra and multidimensional arrays\n",
        "import tensorflow as tf                                 # Deep Learning Tool\n",
        "import os                                               # OS module in Python provides a way of using operating system dependent functionality\n",
        "import cv2                                              # Library for image processing\n",
        "from sklearn.model_selection import train_test_split    # For splitting the data into train and validation set\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAXqCpedduVx"
      },
      "source": [
        "## Loading and preparing training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMXmX8g3dflK"
      },
      "source": [
        "labels = pd.read_csv(\"/content/content/eye_gender_data/Training_set.csv\")   # loading the labels\n",
        "file_paths = [[fname, '/content/content/eye_gender_data/train/' + fname] for fname in labels['filename']]\n",
        "images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
        "train_data = pd.merge(images, labels, how = 'inner', on = 'filename')\n",
        "\n",
        "data = []     # initialize an empty numpy array\n",
        "image_size = 100      # image size taken is 100 here. one can take other size too\n",
        "for i in range(len(train_data)):\n",
        "  \n",
        "  img_array = cv2.imread(train_data['filepaths'][i], cv2.IMREAD_GRAYSCALE)   # converting the image to gray scale\n",
        "\n",
        "  new_img_array = cv2.resize(img_array, (image_size, image_size))      # resizing the image array\n",
        "  data.append([new_img_array, train_data['label'][i]])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FldofLw1fCxI"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvIxaRD6sn3m"
      },
      "source": [
        "## Data Pre-processing\n",
        "It is necessary to bring all the images in the same shape and size, also convert them to their pixel values because all machine learning or deep learning models accepts only the numerical data. Also we need to convert all the labels from categorical to numerical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2QxXk2zik3T",
        "outputId": "6d8d10bc-c19c-4ddf-d523-a8479d61f30a"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[188, 188, 189, ..., 176, 175, 175],\n",
              "        [189, 189, 188, ..., 174, 173, 172],\n",
              "        [190, 189, 188, ..., 168, 167, 167],\n",
              "        ...,\n",
              "        [133, 137, 144, ..., 168, 167, 166],\n",
              "        [134, 138, 145, ..., 165, 164, 163],\n",
              "        [135, 139, 146, ..., 163, 162, 162]], dtype=uint8), 'male']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKwlH181Qgqi",
        "outputId": "7321b20d-e285-45e9-88a8-1bfe288f3921"
      },
      "source": [
        "type(data[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK6UHwJiQgqi",
        "outputId": "71515c3e-5233-409f-e54a-5ddcda68156d"
      },
      "source": [
        "input_shape = data[0][0].shape\n",
        "input_shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiw-4zDahMWT"
      },
      "source": [
        "train_images=[]\n",
        "train_labels=[]\n",
        "for example in data:\n",
        "  ar = example[0]\n",
        "  ar = ar/255\n",
        "  ar = ar.reshape((image_size,image_size,1))\n",
        "  train_images.append(ar)\n",
        "  label = example[1]\n",
        "  if label=='male':\n",
        "    label = 1\n",
        "  else: \n",
        "    label = 0\n",
        "  train_labels.append(label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF10P74ZPiZP",
        "outputId": "42b00984-166d-4e73-fea2-31475a4f10f2"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.7372549 ],\n",
              "        [0.7372549 ],\n",
              "        [0.74117647],\n",
              "        ...,\n",
              "        [0.69019608],\n",
              "        [0.68627451],\n",
              "        [0.68627451]],\n",
              "\n",
              "       [[0.74117647],\n",
              "        [0.74117647],\n",
              "        [0.7372549 ],\n",
              "        ...,\n",
              "        [0.68235294],\n",
              "        [0.67843137],\n",
              "        [0.6745098 ]],\n",
              "\n",
              "       [[0.74509804],\n",
              "        [0.74117647],\n",
              "        [0.7372549 ],\n",
              "        ...,\n",
              "        [0.65882353],\n",
              "        [0.65490196],\n",
              "        [0.65490196]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.52156863],\n",
              "        [0.5372549 ],\n",
              "        [0.56470588],\n",
              "        ...,\n",
              "        [0.65882353],\n",
              "        [0.65490196],\n",
              "        [0.65098039]],\n",
              "\n",
              "       [[0.5254902 ],\n",
              "        [0.54117647],\n",
              "        [0.56862745],\n",
              "        ...,\n",
              "        [0.64705882],\n",
              "        [0.64313725],\n",
              "        [0.63921569]],\n",
              "\n",
              "       [[0.52941176],\n",
              "        [0.54509804],\n",
              "        [0.57254902],\n",
              "        ...,\n",
              "        [0.63921569],\n",
              "        [0.63529412],\n",
              "        [0.63529412]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jcpG6YwTNIq"
      },
      "source": [
        "train_images = np.array(train_images)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP--tOIrTTWk",
        "outputId": "b355bdb0-010b-4bba-cdcd-41d9630dca93"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9220, 100, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVJLUL9dhGBO",
        "outputId": "ae135869-3945-441d-8292-610fd8afc7c0"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.7372549 ],\n",
              "        [0.7372549 ],\n",
              "        [0.74117647],\n",
              "        ...,\n",
              "        [0.69019608],\n",
              "        [0.68627451],\n",
              "        [0.68627451]],\n",
              "\n",
              "       [[0.74117647],\n",
              "        [0.74117647],\n",
              "        [0.7372549 ],\n",
              "        ...,\n",
              "        [0.68235294],\n",
              "        [0.67843137],\n",
              "        [0.6745098 ]],\n",
              "\n",
              "       [[0.74509804],\n",
              "        [0.74117647],\n",
              "        [0.7372549 ],\n",
              "        ...,\n",
              "        [0.65882353],\n",
              "        [0.65490196],\n",
              "        [0.65490196]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.52156863],\n",
              "        [0.5372549 ],\n",
              "        [0.56470588],\n",
              "        ...,\n",
              "        [0.65882353],\n",
              "        [0.65490196],\n",
              "        [0.65098039]],\n",
              "\n",
              "       [[0.5254902 ],\n",
              "        [0.54117647],\n",
              "        [0.56862745],\n",
              "        ...,\n",
              "        [0.64705882],\n",
              "        [0.64313725],\n",
              "        [0.63921569]],\n",
              "\n",
              "       [[0.52941176],\n",
              "        [0.54509804],\n",
              "        [0.57254902],\n",
              "        ...,\n",
              "        [0.63921569],\n",
              "        [0.63529412],\n",
              "        [0.63529412]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1PAZszQP7_R"
      },
      "source": [
        "train_labels = np.array(train_labels)\n",
        "# train_labels.reshape(9220,1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVvY4a0xSmup",
        "outputId": "1b83e4ae-2031-43c7-a412-56d5877e28dc"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9220, 100, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiuLgtlwRn_g"
      },
      "source": [
        "train_images = train_images.reshape(9220,100,100)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDCZ53yE7340"
      },
      "source": [
        "## Building Model & Hyperparameter tuning\n",
        "Now we are finally ready, and we can train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COpBZv4AqETx",
        "outputId": "2ea55db8-7682-469d-af7d-c74a4eee4e25"
      },
      "source": [
        "train_images_3ch = np.stack([train_images]*3, axis=-1)\n",
        "# test_images_3ch = np.stack([test_images]*3, axis=-1)\n",
        "\n",
        "print('\\nTrain_images.shape: {}, of {}'.format(train_images_3ch.shape, train_images_3ch.dtype))\n",
        "# print('Test_images.shape: {}, of {}'.format(test_images_3ch.shape, test_images_3ch.dtype))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train_images.shape: (9220, 100, 100, 3), of float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhZ0rlEEqGm_"
      },
      "source": [
        "## Resizing Image Data for Modeling\n",
        "\n",
        "The minimum image size expected by the VGG model is 32x32 so we need to resize our images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzyKQaYJqLcO"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def resize_image_array(img, img_size_dims):\n",
        "    img = cv2.resize(img, dsize=img_size_dims, \n",
        "                     interpolation=cv2.INTER_CUBIC)\n",
        "    img = np.array(img, dtype=np.float32)\n",
        "    return img"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDqdYoMjqQZg",
        "outputId": "849f0f4b-a757-49b8-cc9b-cf323369cd9b"
      },
      "source": [
        "%%time\n",
        "\n",
        "IMG_DIMS = (32, 32)\n",
        "\n",
        "train_images_3ch = np.array([resize_image_array(img, img_size_dims=IMG_DIMS) for img in train_images_3ch])\n",
        "# test_images_3ch = np.array([resize_image_array(img, img_size_dims=IMG_DIMS) for img in test_images_3ch])\n",
        "\n",
        "print('\\nTrain_images.shape: {}, of {}'.format(train_images_3ch.shape, train_images_3ch.dtype))\n",
        "# print('Test_images.shape: {}, of {}'.format(test_images_3ch.shape, test_images_3ch.dtype))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train_images.shape: (9220, 32, 32, 3), of float32\n",
            "CPU times: user 657 ms, sys: 111 ms, total: 769 ms\n",
            "Wall time: 772 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLNlDxweqTFD"
      },
      "source": [
        "## Build CNN Model Architecture\n",
        "\n",
        "We will now build our CNN model architecture customizing the VGG-19 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyWXrAFtrfkm"
      },
      "source": [
        "### Build Cut-VGG19 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkWEAB5cqw0z",
        "outputId": "8ba4c143-e201-4d9e-d0dc-489bf6e961e9"
      },
      "source": [
        "# define input shape\n",
        "INPUT_SHAPE = (32, 32, 3)\n",
        "\n",
        "# get the VGG19 model\n",
        "vgg_layers = tf.keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, \n",
        "                                               input_shape=INPUT_SHAPE)\n",
        "\n",
        "vgg_layers.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 20,024,384\n",
            "Trainable params: 20,024,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVIxVkRLqhNO",
        "outputId": "eac01533-03f0-4919-c3d0-7cbf9f631c71"
      },
      "source": [
        "# define sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Add the vgg convolutional base model\n",
        "model.add(vgg_layers)\n",
        "\n",
        "# add flatten layer\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# add dense layers with some dropout\n",
        "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.6))\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.6))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "# model.add(tf.keras.layers.Dropout(rate=0.3))\n",
        "# model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "\n",
        "# add output layer\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# view model layers\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg19 (Functional)           (None, 1, 1, 512)         20024384  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 21,206,338\n",
            "Trainable params: 21,206,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQWMyQLva53K",
        "outputId": "1cf5f14d-8e06-4d50-ee5f-ad511b9c9775"
      },
      "source": [
        "EPOCHS = 100\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, \n",
        "                                               restore_best_weights=True,\n",
        "                                               verbose=1)\n",
        "\n",
        "history = model.fit(train_images_3ch, train_labels,\n",
        "                    batch_size=128,\n",
        "                    callbacks=[es_callback], \n",
        "                    validation_split=0.1, epochs=EPOCHS,\n",
        "                    verbose=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "65/65 [==============================] - 44s 199ms/step - loss: 0.6714 - accuracy: 0.6135 - val_loss: 0.4569 - val_accuracy: 0.8026\n",
            "Epoch 2/100\n",
            "65/65 [==============================] - 10s 155ms/step - loss: 0.4124 - accuracy: 0.8208 - val_loss: 0.3396 - val_accuracy: 0.8547\n",
            "Epoch 3/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.2780 - accuracy: 0.8883 - val_loss: 0.2738 - val_accuracy: 0.8894\n",
            "Epoch 4/100\n",
            "65/65 [==============================] - 10s 159ms/step - loss: 0.2141 - accuracy: 0.9164 - val_loss: 0.2456 - val_accuracy: 0.9067\n",
            "Epoch 5/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.1785 - accuracy: 0.9312 - val_loss: 0.2632 - val_accuracy: 0.9078\n",
            "Epoch 6/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.1449 - accuracy: 0.9443 - val_loss: 0.2407 - val_accuracy: 0.9154\n",
            "Epoch 7/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.1292 - accuracy: 0.9513 - val_loss: 0.1955 - val_accuracy: 0.9273\n",
            "Epoch 8/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.1220 - accuracy: 0.9525 - val_loss: 0.2779 - val_accuracy: 0.9035\n",
            "Epoch 9/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.1102 - accuracy: 0.9588 - val_loss: 0.1739 - val_accuracy: 0.9349\n",
            "Epoch 10/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.0855 - accuracy: 0.9702 - val_loss: 0.1852 - val_accuracy: 0.9403\n",
            "Epoch 11/100\n",
            "65/65 [==============================] - 10s 154ms/step - loss: 0.0802 - accuracy: 0.9707 - val_loss: 0.1907 - val_accuracy: 0.9338\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc3lSJ2a-OYt"
      },
      "source": [
        "## Validate the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLfoImV7ds57"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLrpm-If-lRT"
      },
      "source": [
        "## Predict The Output For Testing Dataset üòÖ\n",
        "We have trained our model, evaluated it and now finally we will predict the output/target for the testing data (i.e. Test.csv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG70J6Un-s2G"
      },
      "source": [
        "#### Load Test Set\n",
        "Load the test data on which final submission is to be made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG5SO355WN2F"
      },
      "source": [
        "labels = pd.read_csv(\"/content/content/eye_gender_data/Testing_set.csv\")   # loading the labels\n",
        "file_paths = [[fname, '/content/content/eye_gender_data/test/' + fname] for fname in labels['filename']]\n",
        "images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
        "test_data = pd.merge(images, labels, how = 'inner', on = 'filename')\n",
        "\n",
        "data = []     # initialize an empty numpy array\n",
        "image_size = 100     # image size taken is 100 here. one can take other size too\n",
        "for i in range(len(test_data)):\n",
        "  img_array = cv2.imread(test_data['filepaths'][i], cv2.IMREAD_GRAYSCALE)   # converting the image to gray scale\n",
        "  new_img_array = cv2.resize(img_array, (image_size, image_size))      # resizing the image array\n",
        "  data.append([new_img_array])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqdNk6QoE96-",
        "outputId": "37736ccf-4bfc-4d7b-a6a8-efc1dba7cca4"
      },
      "source": [
        "data[0][0].shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqSujaW0CRi9"
      },
      "source": [
        "## Data Pre-processing on test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPgpnldJ2em3"
      },
      "source": [
        "test_images=[]\n",
        "for example in data:\n",
        "  ar = example[0]\n",
        "  ar = ar/255\n",
        "  ar = ar.reshape((image_size,image_size,1))\n",
        "  test_images.append(ar)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM1T5Sip3lmr",
        "outputId": "be848299-25e7-4e86-e701-fa87179c1187"
      },
      "source": [
        "test_images[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.54901961],\n",
              "        [0.55294118],\n",
              "        [0.55686275],\n",
              "        ...,\n",
              "        [0.6627451 ],\n",
              "        [0.67058824],\n",
              "        [0.67843137]],\n",
              "\n",
              "       [[0.54117647],\n",
              "        [0.54117647],\n",
              "        [0.54117647],\n",
              "        ...,\n",
              "        [0.65098039],\n",
              "        [0.65882353],\n",
              "        [0.66666667]],\n",
              "\n",
              "       [[0.5372549 ],\n",
              "        [0.52941176],\n",
              "        [0.5254902 ],\n",
              "        ...,\n",
              "        [0.63921569],\n",
              "        [0.64313725],\n",
              "        [0.65098039]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.57254902],\n",
              "        [0.58823529],\n",
              "        [0.60784314],\n",
              "        ...,\n",
              "        [0.59215686],\n",
              "        [0.59215686],\n",
              "        [0.59215686]],\n",
              "\n",
              "       [[0.57254902],\n",
              "        [0.59215686],\n",
              "        [0.61568627],\n",
              "        ...,\n",
              "        [0.58039216],\n",
              "        [0.57647059],\n",
              "        [0.57254902]],\n",
              "\n",
              "       [[0.57647059],\n",
              "        [0.59607843],\n",
              "        [0.61960784],\n",
              "        ...,\n",
              "        [0.57647059],\n",
              "        [0.56862745],\n",
              "        [0.56078431]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CnGJSZy4RJq",
        "outputId": "84958f8b-5b2b-487c-8b37-b24a30c16f4d"
      },
      "source": [
        "test_images = np.array(test_images)\n",
        "test_images.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2305, 100, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KNDswlffvYq"
      },
      "source": [
        "test_images = test_images.reshape(2305,100,100)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyPf5GUjXWfl",
        "outputId": "37c62183-be16-48cc-d22d-72f486243392"
      },
      "source": [
        "test_images_3ch = np.stack([test_images]*3, axis=-1)\n",
        "print('Test_images.shape: {}, of {}'.format(test_images_3ch.shape, test_images_3ch.dtype))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test_images.shape: (2305, 100, 100, 3), of float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI3JmlDGXWfo"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def resize_image_array(img, img_size_dims):\n",
        "    img = cv2.resize(img, dsize=img_size_dims, \n",
        "                     interpolation=cv2.INTER_CUBIC)\n",
        "    img = np.array(img, dtype=np.float32)\n",
        "    return img"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbW1hfEWXWfp",
        "outputId": "879203b9-25ca-4c4b-db03-3b545a603fba"
      },
      "source": [
        "%%time\n",
        "\n",
        "IMG_DIMS = (32, 32)\n",
        "\n",
        "test_images_3ch = np.array([resize_image_array(img, img_size_dims=IMG_DIMS) for img in test_images_3ch])\n",
        "\n",
        "print('Test_images.shape: {}, of {}'.format(test_images_3ch.shape, test_images_3ch.dtype))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test_images.shape: (2305, 32, 32, 3), of float32\n",
            "CPU times: user 170 ms, sys: 9.93 ms, total: 180 ms\n",
            "Wall time: 181 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxtDS6-0J0s2"
      },
      "source": [
        "### Make Prediction on Test Dataset\n",
        "Time to make a submission!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G7GpEJL3c5k"
      },
      "source": [
        "predictions  = model.predict(test_images_3ch)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2IvTsakQdJE",
        "outputId": "894af264-931b-4421-d7e1-9bb3abd793e1"
      },
      "source": [
        "predictions[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00192001, 0.99808   ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpg1hcig4FgY"
      },
      "source": [
        "predictions = np.array(predictions)\n",
        "result = np.argmax(predictions,axis=1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeeGGAqb4cad",
        "outputId": "a60b1602-ec46-44b2-d117-3d63abd3752d"
      },
      "source": [
        "result[0]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIu6mwmS6QEU",
        "outputId": "acc16e0c-7227-4c03-ccf8-bae2402dd5d1"
      },
      "source": [
        "len(result)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2305"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31EofuXQ6Ss2",
        "outputId": "0266dc3f-3a98-4294-9b3e-41d5df10c9ba"
      },
      "source": [
        "len(test_images)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2305"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irLqKH6F8LeT"
      },
      "source": [
        "results=[]\n",
        "for value in result:\n",
        "  if value:\n",
        "    results.append('male')\n",
        "  else:\n",
        "    results.append('female')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ubawY66z82X-",
        "outputId": "1aee7d1e-b61e-4384-f28e-f851826142b4"
      },
      "source": [
        "results[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'male'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYeK05BHLAen"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THaqbN4LM6YX"
      },
      "source": [
        "## **How to save prediciton results locally via jupyter notebook?**\n",
        "If you are working on Jupyter notebook, execute below block of codes. A file named 'submission.csv' will be created in your current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I2wAC5RMwvJ"
      },
      "source": [
        "# res = pd.DataFrame({'filename': test_images['filename'], 'label': predictions})  # prediction is nothing but the final predictions of your model on input features of your new unseen test data\n",
        "# res.to_csv(\"submission.csv\", index = False)      # the csv file will be saved locally on the same location where this notebook is located."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fquFjyl6OfGz"
      },
      "source": [
        "# **OR,**\n",
        "**If you are working on Google Colab then use the below set of code to save prediction results locally**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILu2PfzNOqLx"
      },
      "source": [
        "## **How to save prediction results locally via colab notebook?**\n",
        "If you are working on Google Colab Notebook, execute below block of codes. A file named 'prediction_results' will be downloaded in your system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FpcTFRw7NgXp",
        "outputId": "9235d239-735b-4005-c8e7-92747be1d2d3"
      },
      "source": [
        "res = pd.DataFrame({'label': results})  # prediction is nothing but the final predictions of your model on input features of your new unseen test data\n",
        "\n",
        "res.to_csv(\"TransferLearning_128.csv\", index = False) \n",
        "\n",
        "# To download the csv file locally\n",
        "from google.colab import files        \n",
        "files.download('TransferLearning_128.csv')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a3b73603-1e31-4304-b535-3226aad535e4\", \"TransferLearning_128.csv\", 13649)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohZiMIkO_tX"
      },
      "source": [
        "# **Well Done! üëç**\n",
        "You are all set to make a submission. Let's head to the **[challenge page](https://dphi.tech/challenges/4-week-deep-learning-online-bootcamp-final-assignment-sex-determination-by-morphometry-of-eyes/144/submit)** to make the submission."
      ]
    }
  ]
}